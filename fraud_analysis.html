<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fraud Detection Analysis</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            max-width: 900px;
            margin: auto;
            background: #fff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1, h2, h3, h4 {
            color: #2c3e50;
            border-bottom: 2px solid #e74c3c;
            padding-bottom: 5px;
        }
        h1 {
            text-align: center;
        }
        h3 {
            border-bottom: none;
            padding-bottom: 0;
        }
        code {
            background-color: #ecf0f1;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: 'Courier New', Courier, monospace;
            display: block;
            white-space: pre-wrap;
            word-wrap: break-word;
            padding: 10px;
            border: 1px solid #bdc3c7;
            margin-top: 5px;
            margin-bottom: 5px;
        }
        hr {
            border: 0;
            height: 1px;
            background-color: #ddd;
            margin: 20px 0;
        }
        .explanation {
            margin-top: -10px;
            font-style: italic;
            color: #555;
            padding-bottom: 10px;
        }
    </style>
</head>
<body>

    <div class="container">
        <h1>Project: Fraud Detection Analysis</h1>
        <p>This document provides a detailed, beginner-friendly explanation of the <code>Fraud Detection analysis.ipynb</code> Jupyter Notebook. It breaks down each step of the machine learning pipeline, from data preparation to model deployment, to help you understand how the fraud detection system was built.</p>
        <hr>

        <h2>1. Project Overview</h2>
        <p>The main goal of this project is to build a machine learning model that can accurately predict if a financial transaction is fraudulent. This is a crucial task for banks and online payment companies to prevent financial losses. The project is a classic <b>binary classification</b> problem, which means the model has to classify each transaction into one of two categories: <code>isFraud</code> (1) or <code>isFraud</code> (0).</p>
        <p>This notebook follows the standard data science workflow:</p>
        <ol>
            <li><b>Data Exploration:</b> Understanding the data to find patterns and problems.</li>
            <li><b>Data Preprocessing:</b> Cleaning and preparing the data for the machine learning model.</li>
            <li><b>Model Building:</b> Training a model to learn from the data.</li>
            <li><b>Evaluation:</b> Checking how well the model performs.</li>
            <li><b>Deployment:</b> Saving the model to use it in a live application (the Streamlit app).</li>
        </ol>
        <hr>

        <h2>2. Detailed Code Explanation</h2>

        <h3>Section 1: Setup and Data Loading</h3>
        <h4>Code:</h4>
        <code>
            import pandas as pd
            import numpy as np
            import matplotlib.pyplot as plt
            import seaborn as sns
        </code>
        <p><b>Explanation:</b> This is like gathering your tools before you start a project. You're importing four essential libraries for data analysis in Python:
            <ul>
                <li><b><code>pandas</code></b>: The most important tool for working with tabular data (like a spreadsheet). It allows you to load, manipulate, and analyze data easily.</li>
                <li><b><code>numpy</code></b>: A powerful library for numerical operations. It is highly efficient for performing calculations on large datasets.</li>
                <li><b><code>matplotlib.pyplot</code></b> and <b><code>seaborn</code></b>: These are your visualization libraries, your "art supplies." They help you create various charts and graphs to understand your data visually. <code>seaborn</code> is built on top of <code>matplotlib</code>, making it easier to create professional-looking plots.</li>
            </ul>
        </p>

        <h4>Code:</h4>
        <code>
            import warnings
            warnings.filterwarnings("ignore")
            sns.set(style="whitegrid")
        </code>
        <p><b>Explanation:</b> This is a bit of digital housecleaning. The first two lines tell Python to ignore minor warning messages that don't affect the code's functionality, keeping your notebook tidy. The third line sets a clean, professional style for all the charts you'll create, with a white background and a grid.</p>

        <h4>Code:</h4>
        <code>
            df = pd.read_csv("D:\\Rakesh ap\\Projects\\Fraud Detection Project\\AIML Dataset.csv")
        </code>
        <p><b>Explanation:</b> This is the crucial step of loading the dataset. The <code>pd.read_csv()</code> command reads your <code>.csv</code> (Comma Separated Values) file from its location on your computer. It converts this data into a <code>DataFrame</code>, which is a table-like structure, and stores it in the variable <code>df</code> (a common shorthand for DataFrame).</p>

        <hr>

        <h3>Section 2: Exploratory Data Analysis (EDA)</h3>
        <h4>Code:</h4>
        <code>df.head()</code>
        <p><b>Explanation:</b> This is your first look at the data. The <code>.head()</code> function displays the first five rows of your DataFrame. This gives you a quick snapshot of the columns, the types of values they contain, and the overall structure of your dataset.</p>

        <h4>Code:</h4>
        <code>df.info()</code>
        <p><b>Explanation:</b> This command provides a summary of the entire DataFrame. It tells you:
            <ul>
                <li>The total number of rows (entries).</li>
                <li>The name of each column.</li>
                <li>The data type of each column (e.g., <code>int64</code> for integers, <code>float64</code> for decimals, and <code>object</code> for text).</li>
                <li>Whether there are any missing values (<code>Non-Null Count</code>).</li>
            </ul>
        </p>

        <h4>Code:</h4>
        <code>df.isnull().sum().sum()</code>
        <p><b>Explanation:</b> This is a quick and effective way to check for any missing information. It calculates the total count of all missing values (represented as <code>NaN</code>). The output <code>0</code> means your dataset is perfectly clean and ready for analysis.</p>

        <h4>Code:</h4>
        <code>df["isFraud"].value_counts()</code> and <code>df["isFlaggedFraud"].value_counts()</code>
        <p><b>Explanation:</b> This code investigates the most important column for this project: <code>isFraud</code>. The <code>value_counts()</code> function counts how many times each unique value appears in the column. It shows you the number of fraudulent (1) and non-fraudulent (0) transactions. You will notice a huge difference in the counts, which is a common characteristic of fraud datasetsâ€”they are <b>highly imbalanced</b>.</p>

        <h4>Code:</h4>
        <code>round((df["isFraud"].value_counts()[1]/df.shape[0])*100,2)</code>
        <p><b>Explanation:</b> This line calculates the exact percentage of fraudulent transactions. You are taking the count of fraudulent transactions, dividing it by the total number of transactions, and then multiplying by 100. The result, <code>0.13</code>, highlights just how rare fraudulent cases are in this dataset, which is a key challenge for the model.</p>
        <hr>

        <h3>Section 3: Feature Engineering and Data Preprocessing</h3>
        <h4>Code:</h4>
        <code>
            df['nameDest'] = df['nameDest'].str.slice(0, 1)
            df['nameOrig'] = df['nameOrig'].str.slice(0, 1)
        </code>
        <p><b>Explanation:</b> This step simplifies the data by extracting just the first character from the <code>nameDest</code> and <code>nameOrig</code> columns. For example, 'C' might stand for a customer account and 'M' for a merchant. This could potentially give the model a useful feature to work with.</p>

        <h4>Code:</h4>
        <code>
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            df['type'] = le.fit_transform(df['type'])
            df['nameDest'] = le.fit_transform(df['nameDest'])
            df['nameOrig'] = le.fit_transform(df['nameOrig'])
        </code>
        <p><b>Explanation:</b> Machine learning models can only work with numbers, not text. This code uses <code>LabelEncoder</code> to convert the text values in the <code>type</code>, <code>nameDest</code>, and <code>nameOrig</code> columns into numerical representations. For example, 'PAYMENT' might become 0, 'CASH_OUT' might become 1, and so on.</p>

        <h4>Code:</h4>
        <code>
            X = df.drop("isFraud", axis=1)
            y = df["isFraud"]
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        </code>
        <p><b>Explanation:</b> This is a critical step for preparing your model for a fair test.
            <ul>
                <li><code>X</code> contains your <b>features</b> (the input data, or all columns except <code>isFraud</code>).</li>
                <li><code>y</code> contains your <b>target</b> (the output you want to predict, which is the <code>isFraud</code> column).</li>
                <li><code>train_test_split()</code> then divides your data into a <b>training set</b> (80% of the data, used to teach the model) and a <b>testing set</b> (20%, used to see how well the model works on data it has never seen).</li>
            </ul>
        </p>
        <hr>

        <h3>Section 4: Building and Evaluating the Model</h3>
        <h4>Code:</h4>
        <code>
            from sklearn.pipeline import Pipeline
            from sklearn.preprocessing import StandardScaler, OneHotEncoder
            from sklearn.linear_model import LogisticRegression
            # ... code to define the pipeline ...
            pipeline.fit(X_train, y_train)
        </code>
        <p><b>Explanation:</b> A <code>Pipeline</code> is a great way to combine all the data preprocessing steps and the model into a single, neat package. It ensures that the same transformations (like converting text to numbers and scaling the data) are applied consistently to both your training and testing data. The <code>pipeline.fit()</code> command trains the entire pipeline on your training data.</p>

        <h4>Code:</h4>
        <code>
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
            y_pred = pipeline.predict(X_test)
            # ... code to print metrics ...
        </code>
        <p><b>Explanation:</b> Once the model is trained, you need to check its performance.
            <ul>
                <li><code>pipeline.predict(X_test)</code> tells the model to make predictions on the test data it has never seen before.</li>
                <li>The rest of the code calculates various <b>metrics</b> to evaluate the model's performance. For a fraud detection model, <b>precision</b> and <b>recall</b> are more important than accuracy because of the imbalanced data.</li>
            </ul>
        </p>
        <hr>

        <h3>Section 5: Saving the Model for Deployment</h3>
        <h4>Code:</h4>
        <code>
            import joblib
            joblib.dump(pipeline, 'fraud_detection_pipeline.pkl')
        </code>
        <p><b>Explanation:</b> This is the final step where you save your trained model and all the preprocessing steps to a single file. The <code>joblib</code> library is a simple and efficient way to save (<code>dump</code>) the entire machine learning pipeline (<code>pipeline</code>) to a file named <code>fraud_detection_pipeline.pkl</code>. This <code>.pkl</code> file can then be loaded by your Streamlit application, so you don't have to retrain the model every time the app runs.</p>
        <hr>

        <h2>How to Use This App & My Key Learnings</h2>
        <h3>How to Use This App</h3>
        <p>The live Streamlit application simplifies the model into a user-friendly interface. All the complex data preprocessing and prediction logic are handled behind the scenes.</p>
        <ol>
            <li><b>Enter Transaction Details:</b> On the app's home page, you will see several input fields. You need to enter the details of a transaction, such as the <code>Transaction Type</code>, <code>Amount</code>, and the balances of the sender and receiver accounts.</li>
            <li><b>Click "Predict":</b> Once you've entered the information, click the "Predict" button.</li>
            <li><b>Get the Result:</b> The app will use the saved model (<code>fraud_detection_pipeline.pkl</code>) to instantly analyze your input data and tell you if the transaction is likely to be fraudulent or not. The output will be a simple "1" for fraud or "0" for not fraud.</li>
        </ol>
        <h3>My Key Learnings</h3>
        <p>This project was a great learning experience. Here are some of the most important takeaways:</p>
        <ul>
            <li><b>The Importance of EDA:</b> You can't just jump into building a model. The exploratory data analysis phase was crucial for discovering that the data was highly imbalanced. This understanding guided my choice of evaluation metrics (focusing on precision and recall instead of just accuracy).</li>
            <li><b>The Power of Pipelines:</b> Using <code>sklearn.pipeline</code> was a game-changer. It taught me how to package all my data preprocessing steps and the model into a single, reliable object. This makes the code much cleaner and prevents common errors that occur when deploying models.</li>
            <li><b>The Real-World Challenge of Deployment:</b> Building the model is only half the battle. This project showed me the challenges of taking a model from a notebook to a live application. I learned about the importance of using a <code>requirements.txt</code> file to ensure the live environment has the exact same library versions as the one used for training the model. This is a common and critical step for any data science project.</li>
        </ul>
    </div>

</body>
</html>